{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2023-11-23T03:42:59.108616Z","iopub.status.busy":"2023-11-23T03:42:59.108083Z","iopub.status.idle":"2023-11-23T03:43:02.729588Z","shell.execute_reply":"2023-11-23T03:43:02.728334Z","shell.execute_reply.started":"2023-11-23T03:42:59.108558Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     /Users/tiexin/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import pandas as pd\n","\n","import nltk\n","\n","nltk.download(\"stopwords\")\n","\n","from nltk.corpus import stopwords\n","\n","import re\n","from nltk.tokenize.toktok import ToktokTokenizer\n","\n","\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":2,"metadata":{"_uuid":"4c593c17588723c0b0b0f19851cb70a8447ced76","scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(156060, 4)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PhraseId</th>\n","      <th>SentenceId</th>\n","      <th>Phrase</th>\n","      <th>Sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>A series of escapades demonstrating the adage ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>A series of escapades demonstrating the adage ...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>A series</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>A</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>series</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   PhraseId  SentenceId                                             Phrase  \\\n","0         1           1  A series of escapades demonstrating the adage ...   \n","1         2           1  A series of escapades demonstrating the adage ...   \n","2         3           1                                           A series   \n","3         4           1                                                  A   \n","4         5           1                                             series   \n","\n","   Sentiment  \n","0          1  \n","1          2  \n","2          2  \n","3          2  \n","4          2  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_table(\"train.tsv\")\n","print(df.shape)\n","df.head(5)"]},{"cell_type":"code","execution_count":3,"metadata":{"_uuid":"7f11c83b1320c8982b36889145f7f770563674a8","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PhraseId</th>\n","      <th>SentenceId</th>\n","      <th>Sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>156060.000000</td>\n","      <td>156060.000000</td>\n","      <td>156060.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>78030.500000</td>\n","      <td>4079.732744</td>\n","      <td>2.063578</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>45050.785842</td>\n","      <td>2502.764394</td>\n","      <td>0.893832</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>39015.750000</td>\n","      <td>1861.750000</td>\n","      <td>2.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>78030.500000</td>\n","      <td>4017.000000</td>\n","      <td>2.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>117045.250000</td>\n","      <td>6244.000000</td>\n","      <td>3.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>156060.000000</td>\n","      <td>8544.000000</td>\n","      <td>4.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            PhraseId     SentenceId      Sentiment\n","count  156060.000000  156060.000000  156060.000000\n","mean    78030.500000    4079.732744       2.063578\n","std     45050.785842    2502.764394       0.893832\n","min         1.000000       1.000000       0.000000\n","25%     39015.750000    1861.750000       2.000000\n","50%     78030.500000    4017.000000       2.000000\n","75%    117045.250000    6244.000000       3.000000\n","max    156060.000000    8544.000000       4.000000"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df.describe()"]},{"cell_type":"code","execution_count":4,"metadata":{"_uuid":"cb6bb97b0f851947dcf341a1de5708a1f2bc64c1","trusted":true},"outputs":[{"data":{"text/plain":["Sentiment\n","2    79582\n","3    32927\n","1    27273\n","4     9206\n","0     7072\n","Name: count, dtype: int64"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df[\"Sentiment\"].value_counts()"]},{"cell_type":"code","execution_count":5,"metadata":{"_uuid":"f000c43d91f68f6668539f089c6a54c5ce3bd819","trusted":true},"outputs":[],"source":["tokenizer = ToktokTokenizer()\n","stopword_list = nltk.corpus.stopwords.words(\"english\")"]},{"cell_type":"code","execution_count":6,"metadata":{"_uuid":"6f6fcafbdadcdcb0c164e37d71fb9d1623f74d0a","trusted":true},"outputs":[],"source":["# Removing the square brackets\n","def remove_between_square_brackets(text):\n","    return re.sub(\"\\[[^]]*\\]\", \"\", text)\n","\n","\n","# Removing the noisy text\n","def denoise_text(text):\n","    text = remove_between_square_brackets(text)\n","    return text\n","\n","\n","# Apply function on review column\n","df[\"Phrase\"] = df[\"Phrase\"].apply(denoise_text)"]},{"cell_type":"code","execution_count":7,"metadata":{"_uuid":"219da72b025121fd98081df50ae0fcaace10cc9d","trusted":true},"outputs":[],"source":["def remove_special_characters(text):\n","    pattern = r\"[^a-zA-z0-9\\s]\"\n","    text = re.sub(pattern, \"\", text)\n","    return text\n","\n","\n","df[\"Phrase\"] = df[\"Phrase\"].apply(remove_special_characters)"]},{"cell_type":"code","execution_count":8,"metadata":{"_uuid":"2295f2946e0ab74c220ad538d0e7adc04d23f697","trusted":true},"outputs":[],"source":["def simple_stemmer(text):\n","    ps = nltk.porter.PorterStemmer()\n","    text = \" \".join([ps.stem(word) for word in text.split()])\n","    return text\n","\n","\n","df[\"Phrase\"] = df[\"Phrase\"].apply(simple_stemmer)"]},{"cell_type":"code","execution_count":9,"metadata":{"_uuid":"5dbff82b4d2d188d8777b273a75d8ac714d38885","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'has', \"you'll\", 'by', 'does', 'not', 'when', 'him', 'where', \"mustn't\", 'she', 'very', 'y', 'or', 'd', 'those', 'me', 'itself', 't', 'them', \"shouldn't\", 'had', \"couldn't\", 'haven', \"needn't\", 'so', 'mightn', 'for', \"aren't\", \"wouldn't\", \"you've\", 'on', 'won', 'who', 'is', 'just', 'should', 'hers', 'its', 'over', 'here', 'didn', 'hasn', \"it's\", 'but', 'couldn', 'theirs', 'ourselves', 'than', 'ma', 'doesn', 'did', 'if', 'these', \"isn't\", \"mightn't\", \"shan't\", 'will', 'which', 'be', 'down', 'shan', \"she's\", 'aren', 'how', 'it', 'their', 'from', 'm', 'own', 'isn', 'some', 'up', \"hadn't\", 'because', 'this', 'through', 'ours', \"wasn't\", 'too', 'themselves', 'wouldn', 'about', 'each', 'most', \"weren't\", \"hasn't\", 'why', 'being', \"you're\", 'yourselves', 'hadn', \"should've\", 'against', 'needn', 'until', 'was', 'wasn', 'my', 'and', 'then', 'doing', 'we', 'off', 'after', \"you'd\", 'do', 'before', 'such', 'his', 'whom', 'while', 'there', 'weren', 'any', 'few', 'of', 'nor', 'above', 'a', 'between', 'yourself', 'now', 'you', 'herself', 'during', 'your', \"didn't\", 're', 'am', 'with', 'been', 'our', 'were', 'in', 'himself', 's', \"don't\", 'other', 'below', 'to', 'll', 'into', 'shouldn', 'at', 'yours', 'don', \"haven't\", 'i', 'out', 'again', 'having', 'no', 'same', 'under', 'further', 'myself', 'o', 'he', 'are', 'can', 'the', 'more', 'ain', 'what', \"that'll\", \"won't\", 'mustn', 'an', 've', 'her', \"doesn't\", 'once', 'as', 'have', 'they', 'that', 'all', 'only', 'both'}\n"]}],"source":["stop = set(stopwords.words(\"english\"))\n","print(stop)\n","\n","\n","def remove_stopwords(text, is_lower_case=False):\n","    tokens = tokenizer.tokenize(text)\n","    tokens = [token.strip() for token in tokens]\n","    if is_lower_case:\n","        filtered_tokens = [token for token in tokens if token not in stopword_list]\n","    else:\n","        filtered_tokens = [\n","            token for token in tokens if token.lower() not in stopword_list\n","        ]\n","    filtered_text = \" \".join(filtered_tokens)\n","    return filtered_text\n","\n","\n","df[\"Phrase\"] = df[\"Phrase\"].apply(remove_stopwords)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["df = df[df[\"Phrase\"] != \"\"]"]},{"cell_type":"code","execution_count":11,"metadata":{"_kg_hide-output":true,"_uuid":"b20c242bd091929ca896ea2c6e936ca00efe6ecf","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PhraseId</th>\n","      <th>SentenceId</th>\n","      <th>Phrase</th>\n","      <th>Sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>seri escapad demonstr adag good goos also good...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>seri escapad demonstr adag good goos</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>seri</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>seri</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>escapad demonstr adag good goos</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   PhraseId  SentenceId                                             Phrase  \\\n","0         1           1  seri escapad demonstr adag good goos also good...   \n","1         2           1               seri escapad demonstr adag good goos   \n","2         3           1                                               seri   \n","4         5           1                                               seri   \n","5         6           1                    escapad demonstr adag good goos   \n","\n","   Sentiment  \n","0          1  \n","1          2  \n","2          2  \n","4          2  \n","5          2  "]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["import re\n","\n","# import shutil\n","import string\n","\n","import zipfile\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import tensorflow as tf\n","from keras import layers\n","\n","# from keras import losses\n","# from keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["phrases = df[\"Phrase\"].values\n","labels = df[\"Sentiment\"].values\n","dataset = tf.data.Dataset.from_tensor_slices((phrases, labels))\n","\n","batch_size = 256\n","seed = 42\n","\n","train_df, val_df = train_test_split(df, test_size=0.2, random_state=seed)\n","raw_train_ds = tf.data.Dataset.from_tensor_slices(\n","    (train_df[\"Phrase\"], train_df[\"Sentiment\"])\n",").batch(batch_size)\n","raw_val_ds = tf.data.Dataset.from_tensor_slices(\n","    (val_df[\"Phrase\"], val_df[\"Sentiment\"])\n",").batch(batch_size)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["max_features = 10000\n","sequence_length = 250\n","\n","vectorize_layer = layers.TextVectorization(\n","    standardize=\"lower_and_strip_punctuation\",\n","    max_tokens=max_features,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length,\n",")"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# Make a text-only dataset (without labels), then call adapt\n","train_text = raw_train_ds.map(lambda x, y: x)\n","vectorize_layer.adapt(train_text)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def vectorize_text(text, label):\n","    text = tf.expand_dims(text, -1)\n","    return vectorize_layer(text), label"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["train_ds = raw_train_ds.map(vectorize_text)\n","val_ds = raw_val_ds.map(vectorize_text)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["AUTOTUNE = tf.data.AUTOTUNE\n","train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n","val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, None, 32)          320000    \n","                                                                 \n"," dropout (Dropout)           (None, None, 32)          0         \n","                                                                 \n"," global_average_pooling1d (  (None, 32)                0         \n"," GlobalAveragePooling1D)                                         \n","                                                                 \n"," dropout_1 (Dropout)         (None, 32)                0         \n","                                                                 \n"," dense (Dense)               (None, 5)                 165       \n","                                                                 \n","=================================================================\n","Total params: 320165 (1.22 MB)\n","Trainable params: 320165 (1.22 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["embedding_dim = 32\n","\n","model = tf.keras.Sequential(\n","    [\n","        layers.Embedding(max_features, embedding_dim),\n","        layers.Dropout(0.2),\n","        layers.GlobalAveragePooling1D(),\n","        layers.Dropout(0.2),\n","        layers.Dense(5),\n","    ]\n",")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","485/485 [==============================] - 4s 8ms/step - loss: 0.7981 - accuracy: 0.6798 - val_loss: 0.9221 - val_accuracy: 0.6397\n","Epoch 2/10\n","485/485 [==============================] - 3s 7ms/step - loss: 0.7985 - accuracy: 0.6794 - val_loss: 0.9222 - val_accuracy: 0.6399\n","Epoch 3/10\n","485/485 [==============================] - 3s 7ms/step - loss: 0.7988 - accuracy: 0.6789 - val_loss: 0.9221 - val_accuracy: 0.6401\n","Epoch 4/10\n","485/485 [==============================] - 3s 7ms/step - loss: 0.7978 - accuracy: 0.6794 - val_loss: 0.9221 - val_accuracy: 0.6403\n","Epoch 5/10\n","485/485 [==============================] - 3s 7ms/step - loss: 0.7985 - accuracy: 0.6790 - val_loss: 0.9221 - val_accuracy: 0.6396\n","Epoch 6/10\n","485/485 [==============================] - 3s 7ms/step - loss: 0.7979 - accuracy: 0.6793 - val_loss: 0.9221 - val_accuracy: 0.6400\n","Epoch 7/10\n","485/485 [==============================] - 3s 7ms/step - loss: 0.7969 - accuracy: 0.6808 - val_loss: 0.9223 - val_accuracy: 0.6397\n","Epoch 8/10\n","485/485 [==============================] - 3s 7ms/step - loss: 0.7979 - accuracy: 0.6796 - val_loss: 0.9222 - val_accuracy: 0.6398\n","Epoch 9/10\n","485/485 [==============================] - 3s 7ms/step - loss: 0.7982 - accuracy: 0.6797 - val_loss: 0.9221 - val_accuracy: 0.6399\n","Epoch 10/10\n","485/485 [==============================] - 3s 7ms/step - loss: 0.7976 - accuracy: 0.6800 - val_loss: 0.9222 - val_accuracy: 0.6401\n"]}],"source":["model.compile(\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    # optimizer=tf.keras.optimizers.legacy.Adam(),\n","    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.0001),\n","    metrics=[\"accuracy\"],\n",")\n","\n","epochs = 10\n","history = model.fit(train_ds, validation_data=val_ds, epochs=epochs)"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["export_model = tf.keras.Sequential(\n","    [vectorize_layer, model, layers.Activation(\"sigmoid\")]\n",")\n","\n","export_model.compile(\n","    loss=tf.losses.SparseCategoricalCrossentropy(from_logits=False),\n","    optimizer=\"adam\",\n","    metrics=[\"accuracy\"],\n",")"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["dftest = pd.read_table(\"test.tsv\")\n","dftest.Phrase = dftest.Phrase.astype(str)\n","dftest.Phrase = dftest.Phrase.apply(denoise_text)\n","dftest.Phrase = dftest.Phrase.apply(remove_special_characters)\n","dftest.Phrase = dftest.Phrase.apply(simple_stemmer)\n","dftest.Phrase = dftest.Phrase.apply(remove_stopwords)\n","\n","# for i in range(len(dftest[\"Phrase\"])):\n","#     if dftest[\"Phrase\"][i] == \"\":\n","#         dftest[\"Phrase\"][i] = \"average\""]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2072/2072 [==============================] - 1s 635us/step\n"]}],"source":["preds = export_model.predict(dftest.Phrase.values)\n","p = np.array([np.argmax(p) for p in preds])\n","new_df = pd.DataFrame({\"PhraseId\": dftest[\"PhraseId\"], \"Sentiment\": p})\n","new_df.to_csv(\"output.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":134715,"sourceId":320111,"sourceType":"datasetVersion"}],"dockerImageVersionId":28120,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":4}
